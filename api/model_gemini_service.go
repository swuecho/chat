package main

import (
	"bufio"
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"

	"github.com/swuecho/chat_backend/llm/gemini"
	"github.com/swuecho/chat_backend/models"
	"github.com/swuecho/chat_backend/sqlc_queries"
)

// Gemini ChatModel implementation
type GeminiChatModel struct {
	h *ChatHandler
}

func (m *GeminiChatModel) Stream(w http.ResponseWriter, chatSession sqlc_queries.ChatSession, chat_compeletion_messages []models.Message, chatUuid string, regenerate bool, stream bool) (*models.LLMAnswer, error) {
	return m.h.chatStreamGemini(w, chatSession, chat_compeletion_messages, chatUuid, regenerate, stream)
}

// Generated by curl-to-Go: https://mholt.github.io/curl-to-go

// curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \
//     -H 'Content-Type: application/json' \
//     -X POST \
//     -d '{
//       "contents": [{
//         "parts":[{
//           "text": "Write a story about a magic backpack."}]}]}' 2> /dev/null

func (h *ChatHandler) chatStreamGemini(w http.ResponseWriter, chatSession sqlc_queries.ChatSession, chat_compeletion_messages []models.Message, chatUuid string, regenerate bool, stream bool) (*models.LLMAnswer, error) {
	chatFiles, err := h.chatfileService.q.ListChatFilesWithContentBySessionUUID(context.Background(), chatSession.Uuid)
	if err != nil {
		RespondWithAPIError(w, ErrInternalUnexpected.WithDetail("Failed to get chat files").WithDebugInfo(err.Error()))
		return nil, err
	}
	payloadBytes, err := gemini.GenGemminPayload(chat_compeletion_messages, chatFiles)
	if err != nil {
		RespondWithAPIError(w, ErrInternalUnexpected.WithDetail("Failed to generate Gemini payload").WithDebugInfo(err.Error()))
		return nil, err
	}

	url := fmt.Sprintf("https://generativelanguage.googleapis.com/v1beta/models/%s:generateContent?key=$GEMINI_API_KEY", chatSession.Model)
	if stream {
		url = fmt.Sprintf("https://generativelanguage.googleapis.com/v1beta/models/%s:streamGenerateContent?alt=sse&key=$GEMINI_API_KEY", chatSession.Model)
	}

	url = os.ExpandEnv(url)

	req, err := http.NewRequest("POST", url, bytes.NewBuffer(payloadBytes))
	if err != nil {
		fmt.Println("Error while creating request: ", err)
		RespondWithAPIError(w, ErrInternalUnexpected.WithDetail("Failed to create Gemini API request").WithDebugInfo(err.Error()))
		return nil, err
	}
	req.Header.Set("Content-Type", "application/json")

	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		fmt.Println("Error while sending request: ", err)
		RespondWithAPIError(w, ErrInternalUnexpected.WithDetail("Failed to send Gemini API request").WithDebugInfo(err.Error()))
		return nil, err
	}
	defer resp.Body.Close()

	answer_id := chatUuid
	if !regenerate {
		answer_id = NewUUID()
	}

	if !stream {
		// Handle non-streaming response
		body, err := io.ReadAll(resp.Body)
		if err != nil {
			RespondWithAPIError(w, ErrInternalUnexpected.WithDetail("Failed to read Gemini response").WithDebugInfo(err.Error()))
			return nil, err
		}
		// body to GeminiResponse
		var geminiResp gemini.ResponseBody
		err = json.Unmarshal(body, &geminiResp)
		if err != nil {
			RespondWithAPIError(w, ErrInternalUnexpected.WithDetail("Failed to parse Gemini response").WithDebugInfo(err.Error()))
			return nil, err
		}
		answer := geminiResp.Candidates[0].Content.Parts[0].Text
		response := constructChatCompletionStreamReponse(answer_id, answer)
		data, _ := json.Marshal(response)
		fmt.Fprint(w, string(data))
		return &models.LLMAnswer{
			Answer:   answer,
			AnswerId: answer_id,
		}, nil
	}

	// Handle streaming response
	setSSEHeader(w)
	flusher, ok := w.(http.Flusher)
	if !ok {
		RespondWithAPIError(w, APIError{
			HTTPCode: http.StatusInternalServerError,
			Code:     "STREAM_UNSUPPORTED",
			Message:  "Streaming unsupported by client",
		})
		return nil, err
	}

	var answer string
	var headerData = []byte("data: ")
	ioreader := bufio.NewReader(resp.Body)
	defer resp.Body.Close()

	count := 0
	for {
		count++
		if count > 10000 {
			break
		}
		line, err := ioreader.ReadBytes('\n')
		if chatSession.Debug {
			log.Printf("%s", line)
		}
		if err != nil {
			// Create an instance of ErrorResponse
			if errors.Is(err, io.EOF) {
				log.Printf("End of stream reached: %+v, %+v", err, line)
				return &models.LLMAnswer{
					Answer:   answer,
					AnswerId: answer_id,
				}, nil
			} else {
				log.Printf("Error while reading response: %+v, %+v", err, line)
				return nil, err
			}
		}
		if !bytes.HasPrefix(line, headerData) {
			continue
		}
		line = bytes.TrimPrefix(line, headerData)
		if len(line) > 0 {
			answer = gemini.ParseRespLine(line, answer)
			data, _ := json.Marshal(constructChatCompletionStreamReponse(answer_id, answer))
			fmt.Fprintf(w, "data: %v\n\n", string(data))
			flusher.Flush()
		}
	}
	return &models.LLMAnswer{
		AnswerId: answer_id,
		Answer:   answer,
	}, nil
}
